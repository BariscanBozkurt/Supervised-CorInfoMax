{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39d372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import repeat\n",
    "from torch.nn.parameter import Parameter\n",
    "import collections\n",
    "import matplotlib\n",
    "from torch_utils import *\n",
    "from models import *\n",
    "from visualization import *\n",
    "# matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f458ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateCorInfoMaxV3(model, loader, neural_lr_start, neural_lr_stop, neural_lr_rule, \n",
    "                         neural_lr_decay_multiplier,\n",
    "                         neural_dynamic_iterations, device, printing = True):\n",
    "    # Evaluate the model on a dataloader with T steps for the dynamics\n",
    "    #model.eval()\n",
    "    correct=0\n",
    "    phase = 'Train' if loader.dataset.train else 'Test'\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x = x.view(x.size(0),-1).to(device).T\n",
    "        #x = 2*x - 1\n",
    "        y = y.to(device)\n",
    "        \n",
    "        neurons = model.fast_forward(x)\n",
    "        \n",
    "        # dynamics for T time steps\n",
    "        neurons = model.run_neural_dynamics(x, y_one_hot, neurons, neural_lr_start, neural_lr_stop, \n",
    "                                            neural_lr_rule,\n",
    "                                            neural_lr_decay_multiplier, neural_dynamic_iterations, 0, \"test\")\n",
    "        pred = torch.argmax(neurons[-1], dim=0).squeeze()  # in this case prediction is done directly on the last (output) layer of neurons\n",
    "        correct += (y == pred).sum().item()\n",
    "\n",
    "    acc = correct/len(loader.dataset) \n",
    "    if printing:\n",
    "        print(phase+' accuracy :\\t', acc)   \n",
    "    return acc\n",
    "\n",
    "# def evaluateCorInfoMaxV4(model, loader, neural_lr_start, neural_lr_stop, neural_lr_rule, \n",
    "#                          neural_lr_decay_multiplier,\n",
    "#                          neural_dynamic_iterations, device, printing = True):\n",
    "#     # Evaluate the model on a dataloader with T steps for the dynamics\n",
    "#     #model.eval()\n",
    "#     correct=0\n",
    "#     phase = 'Train' if loader.dataset.train else 'Test'\n",
    "    \n",
    "#     for x, y in loader:\n",
    "#         x = x.view(x.size(0),-1).to(device).T\n",
    "#         #x = 2*x - 1\n",
    "#         y = y.to(device)\n",
    "        \n",
    "#         neurons = model.fast_forward(x)\n",
    "        \n",
    "# #         # dynamics for T time steps\n",
    "# #         neurons = model.run_neural_dynamics(x, y_one_hot, neurons, neural_lr_start, neural_lr_stop, \n",
    "# #                                             neural_lr_rule,\n",
    "# #                                             neural_lr_decay_multiplier, neural_dynamic_iterations, 0, \"test\")\n",
    "#         pred = torch.argmax(neurons[-1], dim=0).squeeze()  # in this case prediction is done directly on the last (output) layer of neurons\n",
    "#         correct += (y == pred).sum().item()\n",
    "\n",
    "#     acc = correct/len(loader.dataset) \n",
    "#     if printing:\n",
    "#         print(phase+' accuracy :\\t', acc)   \n",
    "#     return acc\n",
    "\n",
    "def evaluateCorInfoMaxV4(model, loader, device, printing = True):\n",
    "    # Evaluate the model on a dataloader with T steps for the dynamics\n",
    "    #model.eval()\n",
    "    correct=0\n",
    "    phase = 'Train' if loader.dataset.train else 'Test'\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x = activation_inverse(x.view(x.size(0),-1).T, \"sigmoid\").to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        neurons = model.fast_forward(x)\n",
    "        \n",
    "#         # dynamics for T time steps\n",
    "#         neurons = model.run_neural_dynamics(x, y_one_hot, neurons, neural_lr_start, neural_lr_stop, \n",
    "#                                             neural_lr_rule,\n",
    "#                                             neural_lr_decay_multiplier, neural_dynamic_iterations, 0, \"test\")\n",
    "        pred = torch.argmax(neurons[-1], dim=0).squeeze()  # in this case prediction is done directly on the last (output) layer of neurons\n",
    "        correct += (y == pred).sum().item()\n",
    "\n",
    "    acc = correct/len(loader.dataset) \n",
    "    if printing:\n",
    "        print(phase+' accuracy :\\t', acc)   \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea1ff3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4018d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), \n",
    "                                            torchvision.transforms.Normalize(mean=(0.0,), std=(1.0,))])\n",
    "\n",
    "mnist_dset_train = torchvision.datasets.MNIST('./data', train=True, transform=transform, target_transform=None, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dset_train, batch_size=20, shuffle=True, num_workers=0)\n",
    "\n",
    "mnist_dset_test = torchvision.datasets.MNIST('./data', train=False, transform=transform, target_transform=None, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_dset_test, batch_size=20, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71519c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation_type = \"sigmoid\"\n",
    "# # activation = F.hardtanh\n",
    "# architecture = [784, 500, 10]\n",
    "\n",
    "# beta = 1\n",
    "# lambda_ = 0.9999\n",
    "# epsilon = 1#0.5\n",
    "# psiv = 0.1\n",
    "# one_over_epsilon = 1 / epsilon\n",
    "# lr_start = {'ff' : 0.01, 'fb': 0.001, 'lat': 1e-3}\n",
    "# neural_lr_start = 0.1\n",
    "# neural_lr_stop = 1e-3\n",
    "# neural_lr_rule = \"divide_by_slow_loop_index\"\n",
    "# neural_lr_decay_multiplier = 0.1\n",
    "# neural_dynamic_iterations = 20\n",
    "# output_sparsity = True\n",
    "# STlambda_lr = 0.005\n",
    "# model = CorInfoMaxErrorProp( architecture = architecture, lambda_ = lambda_,\n",
    "#                             epsilon = epsilon, psiv = psiv, activation_type = activation_type,\n",
    "#                             output_sparsity = output_sparsity, STlambda_lr = STlambda_lr\n",
    "#                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca63f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_type = \"linear\"\n",
    "# activation = F.hardtanh\n",
    "architecture = [784, 500, 10]\n",
    "\n",
    "x,y = next(iter(train_loader))\n",
    "x = x.view(x.size(0),-1).to(device).T\n",
    "y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "\n",
    "beta = 1\n",
    "lambda_ = 1 - 1e-5\n",
    "epsilon = 1#0.5\n",
    "psiv = 0.01\n",
    "one_over_epsilon = 1 / epsilon\n",
    "lr_start = {'ff' : 0.001, 'fb': 0.001, 'lat': 1e-3}\n",
    "neural_lr_start = 0.1\n",
    "neural_lr_stop = 1e-3\n",
    "neural_lr_rule = \"divide_by_slow_loop_index\"\n",
    "neural_lr_decay_multiplier = 0.1\n",
    "neural_dynamic_iterations = 50\n",
    "output_sparsity = True\n",
    "STlambda_lr = 0.005\n",
    "model = CorInfoMaxErrorProp( architecture = architecture, lambda_ = lambda_,\n",
    "                            epsilon = epsilon, psiv = psiv, activation_type = activation_type,\n",
    "                            output_sparsity = output_sparsity, STlambda_lr = STlambda_lr\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a57794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_type = \"linear\"\n",
    "# activation = F.hardtanh\n",
    "architecture = [784, 500, 10]\n",
    "\n",
    "x,y = next(iter(train_loader))\n",
    "x = x.view(x.size(0),-1).to(device).T\n",
    "y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "\n",
    "beta = 1\n",
    "lambda_ = 1 - 1e-3\n",
    "epsilon = 0.1#0.5\n",
    "psiv = 0.1\n",
    "one_over_epsilon = 1 / epsilon\n",
    "lr_start = {'ff' : 0.01, 'fb': 0.003, 'lat': 1e-3}\n",
    "neural_lr_start = 0.05\n",
    "neural_lr_stop = 1e-3\n",
    "neural_lr_rule = \"constant\"\n",
    "neural_lr_decay_multiplier = 0.1\n",
    "neural_dynamic_iterations = 50\n",
    "output_sparsity = True\n",
    "STlambda_lr = 0.005\n",
    "model = CorInfoMaxErrorProp( architecture = architecture, lambda_ = lambda_,\n",
    "                            epsilon = epsilon, psiv = psiv, activation_type = activation_type,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220e6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [01:12, 41.20it/s]\n",
      "4it [00:00, 38.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Train Accuracy : 0.09871666666666666, Test Accuracy : 0.098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [01:12, 41.40it/s]\n"
     ]
    }
   ],
   "source": [
    "trn_acc_list = []\n",
    "tst_acc_list = []\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch_ in range(n_epochs):\n",
    "    lr = {'ff' : lr_start['ff'] * (0.99)**epoch_, 'fb' : lr_start['fb'] * (0.99)**epoch_}\n",
    "    for idx, (x, y) in tqdm(enumerate(train_loader)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x = activation_inverse(x.view(x.size(0),-1).T, activation_type).to(device)\n",
    "        y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "\n",
    "        _ = model.batch_step(x, y_one_hot, lr, neural_lr_start, neural_lr_stop,\n",
    "                             neural_lr_rule, neural_lr_decay_multiplier,\n",
    "                             neural_dynamic_iterations, beta, clip_neural_grad_updates = True, \n",
    "                             optimizer = \"sgd\",\n",
    "                             adam_opt_params = {\"beta1\": 0.9, \"beta2\": 0.999, \"eps\": 1e-8})\n",
    "    trn_acc = evaluateCorInfoMaxV4(model, train_loader, device = 'cuda', printing = False)\n",
    "    tst_acc = evaluateCorInfoMaxV4(model, test_loader, device = 'cuda', printing = False)\n",
    "    trn_acc_list.append(trn_acc)\n",
    "    tst_acc_list.append(tst_acc)\n",
    "    \n",
    "    print(\"Epoch : {}, Train Accuracy : {}, Test Accuracy : {}\".format(epoch_+1, trn_acc, tst_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "496d205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wff_copy = torch.clone(model.Wff[1]['weight'])\n",
    "Wfb_copy = torch.clone(model.Wfb[1]['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12653c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x, y = x.to(device), y.to(device)\n",
    "x = activation_inverse(x.view(x.size(0),-1).T, activation_type).to(device)\n",
    "y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "\n",
    "_ = model.batch_step(x, y_one_hot, lr_start, neural_lr_start, neural_lr_stop,\n",
    "                     neural_lr_rule, neural_lr_decay_multiplier,\n",
    "                     neural_dynamic_iterations, beta, optimizer = \"sgd\",\n",
    "                     adam_opt_params = {\"beta1\": 0.9, \"beta2\": 0.999, \"eps\": 1e-8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e339088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0198, device='cuda:0')\n",
      "tensor(0.0083, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(model.Wff[1][\"weight\"] - Wff_copy))\n",
    "print(torch.norm(model.Wfb[1][\"weight\"] - Wfb_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015e7f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy :\t 0.12658333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12658333333333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateCorInfoMaxV4(model, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d169b549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3000it [03:10, 15.77it/s]\n",
      "2it [00:00, 16.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Train Accuracy : 0.09871666666666666, Test Accuracy : 0.098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:02, 15.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42436/2235239034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         _ = model.batch_step(x, y_one_hot, lr, neural_lr_start, neural_lr_stop,\n\u001b[0m\u001b[1;32m     14\u001b[0m                              \u001b[0mneural_lr_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneural_lr_decay_multiplier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                              \u001b[0mneural_dynamic_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/users/bbozkurt15/CorInfoMaxSupervised/Supervised-CorInfoMax/src/models.py\u001b[0m in \u001b[0;36mbatch_step\u001b[0;34m(self, x, y, lr, neural_lr_start, neural_lr_stop, neural_lr_rule, neural_lr_decay_multiplier, neural_dynamic_iterations, beta, mode, optimizer, adam_opt_params)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;31m# neurons = self.init_neurons(x.size(1), device = self.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mneurons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;31m# if mode == \"train\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;31m#     neurons[-1] = y.to(torch.float)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/users/bbozkurt15/CorInfoMaxSupervised/Supervised-CorInfoMax/src/models.py\u001b[0m in \u001b[0;36mrun_neural_dynamics\u001b[0;34m(self, x, y, neurons, neural_lr_start, neural_lr_stop, lr_rule, lr_decay_multiplier, neural_dynamic_iterations, beta)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlr_rule\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"divide_by_loop_index\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mneural_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_lr_start\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneural_lr_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mlr_rule\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"divide_by_slow_loop_index\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mneural_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneural_lr_start\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlr_decay_multiplier\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneural_lr_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/users/bbozkurt15/CorInfoMaxSupervised/Supervised-CorInfoMax/src/models.py\u001b[0m in \u001b[0;36mcalculate_neural_dynamics_grad\u001b[0;34m(self, x, y, neurons, beta, mode)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mgam_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgam_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mone_over_epsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_over_epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0mpsiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsiv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/users/bbozkurt15/CorInfoMaxSupervised/Supervised-CorInfoMax/src/models.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mgam_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgam_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mone_over_epsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_over_epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0mpsiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpsiv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trn_acc_list = []\n",
    "tst_acc_list = []\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch_ in range(n_epochs):\n",
    "    lr = {'ff' : lr_start['ff'] * (0.99)**epoch_, 'fb' : lr_start['fb'] * (0.99)**epoch_}\n",
    "    for idx, (x, y) in tqdm(enumerate(train_loader)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x = activation_inverse(x.view(x.size(0),-1).T, activation_type).to(device)\n",
    "        y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "\n",
    "        _ = model.batch_step(x, y_one_hot, lr, neural_lr_start, neural_lr_stop,\n",
    "                             neural_lr_rule, neural_lr_decay_multiplier,\n",
    "                             neural_dynamic_iterations, beta, optimizer = \"adam\",\n",
    "                             adam_opt_params = {\"beta1\": 0.9, \"beta2\": 0.999, \"eps\": 1e-8})\n",
    "    trn_acc = evaluateCorInfoMaxV4(model, train_loader, device = 'cuda', printing = False)\n",
    "    tst_acc = evaluateCorInfoMaxV4(model, test_loader, device = 'cuda', printing = False)\n",
    "    trn_acc_list.append(trn_acc)\n",
    "    tst_acc_list.append(tst_acc)\n",
    "    \n",
    "    print(\"Epoch : {}, Train Accuracy : {}, Test Accuracy : {}\".format(epoch_+1, trn_acc, tst_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d7b6e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "               [nan, nan, nan,  ..., nan, nan, nan],\n",
       "               [nan, nan, nan,  ..., nan, nan, nan],\n",
       "               ...,\n",
       "               [nan, nan, nan,  ..., nan, nan, nan],\n",
       "               [nan, nan, nan,  ..., nan, nan, nan],\n",
       "               [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'), 'bias': tensor([[nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan]], device='cuda:0')}                                                      ,\n",
       "       {'weight': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "               [nan, nan, nan,  ..., nan, nan, nan],\n",
       "               [nan, nan, nan,  ..., nan, nan, nan],\n",
       "               ...,\n",
       "               [nan, nan, nan,  ..., nan, nan, nan],\n",
       "               [nan, nan, nan,  ..., nan, nan, nan],\n",
       "               [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'), 'bias': tensor([[nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan],\n",
       "               [nan]], device='cuda:0')}                                                      ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Wff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0778e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d5448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d2a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae229d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_type = \"sigmoid\"\n",
    "# activation = F.hardtanh\n",
    "architecture = [784, 500, 10]\n",
    "\n",
    "x,y = next(iter(train_loader))\n",
    "x = x.view(x.size(0),-1).to(device).T\n",
    "y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "\n",
    "beta = 1\n",
    "lambda_ = 1 - 1e-4\n",
    "epsilon = 1#0.5\n",
    "psiv = 0.01\n",
    "one_over_epsilon = 1 / epsilon\n",
    "lr_start = {'ff' : 0.005, 'fb': 0.001, 'lat': 1e-3}\n",
    "neural_lr_start = 0.1\n",
    "neural_lr_stop = 1e-3\n",
    "neural_lr_rule = \"divide_by_slow_loop_index\"\n",
    "neural_lr_decay_multiplier = 0.1\n",
    "neural_dynamic_iterations = 50\n",
    "output_sparsity = True\n",
    "STlambda_lr = 0.005\n",
    "model = CorInfoMaxErrorProp( architecture = architecture, lambda_ = lambda_,\n",
    "                            epsilon = epsilon, psiv = psiv, activation_type = activation_type,\n",
    "                            output_sparsity = output_sparsity, STlambda_lr = STlambda_lr\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9124a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x, y = x.to(device), y.to(device)\n",
    "# x = x.view(x.size(0),-1).T\n",
    "x = activation_inverse(x.view(x.size(0),-1).T, activation_type).to(device)\n",
    "y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "neurons = model.fast_forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = model.run_neural_dynamics(x, y_one_hot, neurons,neural_lr_start, neural_lr_stop, \n",
    "                                    neural_lr_rule, neural_lr_decay_multiplier, neural_dynamic_iterations)\n",
    "\n",
    "print(torch.argmax(neurons[-1], dim=0).squeeze(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = model.batch_step(x, y_one_hot, lr_start, neural_lr_start, neural_lr_stop, neural_lr_rule,\n",
    "                neural_lr_decay_multiplier, neural_dynamic_iterations)\n",
    "print(torch.argmax(neurons[-1], dim=0).squeeze(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons[-1][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea18647",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_errors = model.calculate_neural_dynamics_grad(x, y_one_hot, neurons, 1)\n",
    "len(backward_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e328967",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_errors[-2][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = model.run_neural_dynamics(x, y_one_hot, neurons,neural_lr_start, neural_lr_stop, \n",
    "                                    neural_lr_rule, neural_lr_decay_multiplier, neural_dynamic_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bffae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = model.run_neural_dynamics(x, y_one_hot, neurons,neural_lr_start, neural_lr_stop, \n",
    "                                    neural_lr_rule, neural_lr_decay_multiplier, neural_dynamic_iterations)\n",
    "\n",
    "print(torch.argmax(neurons[-1], dim=0).squeeze(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = model.calculate_neural_dynamics_grad(x, y, neurons, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(reversed(backward_errors))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95704b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad867a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.Wfb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8bcc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for jj in reversed(range(1, len(model.Wfb))):\n",
    "    print(jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateCorInfoMaxV3(model, test_loader, neural_lr_start, neural_lr_stop, neural_lr_rule, \n",
    "                         neural_lr_decay_multiplier,\n",
    "                         neural_dynamic_iterations, device, printing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_acc_list = []\n",
    "tst_acc_list = []\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch_ in range(n_epochs):\n",
    "    lr = {'ff' : lr_start['ff'] * (0.99)**epoch_, 'fb' : lr_start['fb'] * (0.99)**epoch_}\n",
    "    for idx, (x, y) in tqdm(enumerate(train_loader)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x = x.view(x.size(0),-1).T\n",
    "        #x = 2*x - 1\n",
    "        y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "\n",
    "        _ = model.batch_step(  x, y_one_hot, lr, neural_lr_start, neural_lr_stop, neural_lr_rule,\n",
    "                               neural_lr_decay_multiplier, neural_dynamic_iterations, beta,\n",
    "                               optimizer = \"sgd\")\n",
    "\n",
    "    trn_acc = evaluateCorInfoMaxV3(  model, train_loader, neural_lr_start, neural_lr_stop, neural_lr_rule, \n",
    "                                     neural_lr_decay_multiplier,\n",
    "                                     neural_dynamic_iterations, device, printing = False)\n",
    "    tst_acc = evaluateCorInfoMaxV3(  model, test_loader, neural_lr_start, neural_lr_stop, neural_lr_rule, \n",
    "                                     neural_lr_decay_multiplier,\n",
    "                                     neural_dynamic_iterations, device, printing = False)\n",
    "    trn_acc_list.append(trn_acc)\n",
    "    tst_acc_list.append(tst_acc)\n",
    "    \n",
    "    print(\"Epoch : {}, Train Accuracy : {}, Test Accuracy : {}\".format(epoch_+1, trn_acc, tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_acc = evaluateCorInfoMaxV4(  model, train_loader, neural_lr_start, neural_lr_stop, neural_lr_rule, \n",
    "                                 neural_lr_decay_multiplier,\n",
    "                                 neural_dynamic_iterations, device, printing = False)\n",
    "tst_acc = evaluateCorInfoMaxV4(  model, test_loader, neural_lr_start, neural_lr_stop, neural_lr_rule, \n",
    "                                 neural_lr_decay_multiplier,\n",
    "                                 neural_dynamic_iterations, device, printing = False)\n",
    "trn_acc_list.append(trn_acc)\n",
    "tst_acc_list.append(tst_acc)\n",
    "\n",
    "print(\"Epoch : {}, Train Accuracy : {}, Test Accuracy : {}\".format(epoch_+1, trn_acc, tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad465f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_lr_start = 0.1\n",
    "neural_lr_stop = 1e-3\n",
    "neural_lr_rule = \"divide_by_slow_loop_index\"\n",
    "neural_lr_decay_multiplier = 0.1\n",
    "neural_dynamic_iterations = 50\n",
    "output_sparsity = True\n",
    "STlambda_lr = 0.01\n",
    "\n",
    "trn_acc = evaluateCorInfoMaxV3(  model, train_loader, neural_lr_start, neural_lr_stop, neural_lr_rule, \n",
    "                                 neural_lr_decay_multiplier,\n",
    "                                 neural_dynamic_iterations, device, printing = False)\n",
    "tst_acc = evaluateCorInfoMaxV3(  model, test_loader, neural_lr_start, neural_lr_stop, neural_lr_rule, \n",
    "                                 neural_lr_decay_multiplier,\n",
    "                                 neural_dynamic_iterations, device, printing = False)\n",
    "trn_acc_list.append(trn_acc)\n",
    "tst_acc_list.append(tst_acc)\n",
    "print(\"Epoch : {}, Train Accuracy : {}, Test Accuracy : {}\".format(epoch_+1, trn_acc, tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Epoch : {}, Train Accuracy : {}, Test Accuracy : {}\".format(epoch_+1, trn_acc, tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f8950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f229c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d34ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db52d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727d327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
