{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd7911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import repeat\n",
    "from torch.nn.parameter import Parameter\n",
    "import collections\n",
    "import matplotlib\n",
    "from torch_utils import *\n",
    "from ExplicitModels import *\n",
    "from visualization import *\n",
    "# matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0481fdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c4836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), \n",
    "                                            torchvision.transforms.Normalize(mean=(0.0,), std=(1.0,))])\n",
    "\n",
    "mnist_dset_train = torchvision.datasets.MNIST('./data', train=True, transform=transform, target_transform=None, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dset_train, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "mnist_dset_test = torchvision.datasets.MNIST('./data', train=False, transform=transform, target_transform=None, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_dset_test, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd835cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = F.relu\n",
    "architecture = [784, 128, 64, 10]\n",
    "supervised_lambda_weight = 1e-3\n",
    "neural_lr_start = 0.1 * supervised_lambda_weight\n",
    "neural_lr_stop = 0.05 * supervised_lambda_weight\n",
    "neural_lr_rule = \"constant\"\n",
    "neural_lr_decay_multiplier = 0.005\n",
    "neural_dynamic_iterations = 50\n",
    "\n",
    "model = SupervisedPredictiveCodingNudged_wAutoGrad(architecture, activation, use_stepLR = True, \n",
    "                                                   sgd_nesterov = False, optimizer_type = \"sgd\", \n",
    "                                                   optim_lr = 1e-3, stepLR_step_size = 10*3000,\n",
    "                                                   supervised_lambda_weight = supervised_lambda_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5baa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "357it [00:30, 11.69it/s]"
     ]
    }
   ],
   "source": [
    "trn_acc_list = []\n",
    "tst_acc_list = []\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch_ in range(n_epochs):\n",
    "#     if epoch_ > 12:\n",
    "#         neural_lr_start = 0.05\n",
    "#     if epoch_ > 17:\n",
    "#         neural_lr_start = 0.03\n",
    "    for idx, (x, y) in tqdm(enumerate(train_loader)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x = x.to(device).view(x.size(0),-1).T\n",
    "        y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "        #y_one_hot = 0.94 * y_one_hot + 0.03 * torch.ones(*y_one_hot.shape, device = device)\n",
    "        \n",
    "        model.batch_step(  x, y_one_hot, neural_lr_start, neural_lr_stop, neural_lr_rule,\n",
    "                           neural_lr_decay_multiplier, neural_dynamic_iterations,\n",
    "                        )\n",
    "\n",
    "    trn_acc = evaluatePC(model, train_loader, device, False, \n",
    "                         printing = False)\n",
    "    tst_acc = evaluatePC(model, test_loader, device, False, \n",
    "                         printing = False)\n",
    "    trn_acc_list.append(trn_acc)\n",
    "    tst_acc_list.append(tst_acc)\n",
    "    \n",
    "    print(\"Epoch : {}, Train Accuracy : {}, Test Accuracy : {}\".format(epoch_+1, trn_acc, tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa8752",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_acc = evaluatePC(model, train_loader, device, False, \n",
    "                     printing = False)\n",
    "tst_acc = evaluatePC(model, test_loader, device, False, \n",
    "                     printing = False)\n",
    "trn_acc_list.append(trn_acc)\n",
    "tst_acc_list.append(tst_acc)\n",
    "\n",
    "print(\"Epoch : {}, Train Accuracy : {}, Test Accuracy : {}\".format(epoch_+1, trn_acc, tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca984790",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x, y = x.to(device), y.to(device)\n",
    "y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "x = x.view(x.size(0),-1).T\n",
    "neurons = model.fast_forward(x, no_grad = True)\n",
    "z = torch.clone(neurons[-1])#.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2797b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons[-1].shape, y.shape, z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceaf5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons[-1].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(z.T, 1).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c09568",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.requires_grad_()\n",
    "loss = torch.nn.CrossEntropyLoss(reduction = \"none\")\n",
    "l = loss(F.softmax(z, 0).T, y_one_hot.to(torch.float).T)\n",
    "l.backward()\n",
    "torch.norm(z.grad - (F.softmax(z, 0) - y_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(F.softmax(z, 0) - y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc02292",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad / (F.softmax(z, 0) - y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2102f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(z, 0) - y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe66593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim_params = []\n",
    "# for idx in range(len(model.Wff)):\n",
    "#     for key_ in [\"weight\", \"bias\"]:\n",
    "#         optim_params.append(  {'params': model.Wff[idx][key_], 'lr': lr_start[\"ff\"]}  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7526f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(optim_params, maximize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07503a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_acc_list = []\n",
    "tst_acc_list = []\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch_ in range(n_epochs):\n",
    "    if epoch_ > 12:\n",
    "        neural_lr_start = 0.05\n",
    "    if epoch_ > 17:\n",
    "        neural_lr_start = 0.03\n",
    "    for idx, (x, y) in tqdm(enumerate(train_loader)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x = activation_inverse(x.view(x.size(0),-1).T, \"sigmoid\")\n",
    "        y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "        y_one_hot = 0.94 * y_one_hot + 0.03 * torch.ones(*y_one_hot.shape, device = device)\n",
    "        \n",
    "        model.batch_step(  x, y_one_hot, neural_lr_start, neural_lr_stop, neural_lr_rule,\n",
    "                               neural_lr_decay_multiplier, neural_dynamic_iterations,\n",
    "                               )\n",
    "\n",
    "    trn_acc = evaluatePC(  model, train_loader, neural_lr_start, neural_lr_stop, neural_lr_rule, \n",
    "                           neural_lr_decay_multiplier,\n",
    "                           neural_dynamic_iterations, device, printing = False)\n",
    "    tst_acc = evaluatePC(  model, test_loader, neural_lr_start, neural_lr_stop, neural_lr_rule, \n",
    "                           neural_lr_decay_multiplier,\n",
    "                           neural_dynamic_iterations, device, printing = False)\n",
    "    trn_acc_list.append(trn_acc)\n",
    "    tst_acc_list.append(tst_acc)\n",
    "    \n",
    "    print(\"Epoch : {}, Train Accuracy : {}, Test Accuracy : {}\".format(epoch_+1, trn_acc, tst_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
