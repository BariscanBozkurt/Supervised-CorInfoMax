{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79cee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from models import *\n",
    "from torch_utils import *\n",
    "from visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba25e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a32479",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), \n",
    "                                            torchvision.transforms.Normalize(mean=(0.0,), std=(1.0,))])\n",
    "\n",
    "mnist_dset_train = torchvision.datasets.MNIST('./data', train=True, transform=transform, target_transform=None, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dset_train, batch_size=20, shuffle=True, num_workers=0)\n",
    "\n",
    "mnist_dset_test = torchvision.datasets.MNIST('./data', train=False, transform=transform, target_transform=None, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_dset_test, batch_size=20, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414f8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = hard_sigmoid\n",
    "criterion = torch.nn.MSELoss(reduction='none').to(device)\n",
    "\n",
    "architecture = [784, 500, 10]\n",
    "\n",
    "x,y = next(iter(train_loader))\n",
    "x = x.view(x.size(0),-1).to(device).T\n",
    "y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "\n",
    "lambda_h = 0.99\n",
    "lambda_y = 0.99\n",
    "epsilon = 0.1\n",
    "one_over_epsilon = 1 / epsilon\n",
    "lr = {'ff' : 0.05, 'fb': 0.05, 'lat': 1e-3}\n",
    "neural_lr = 0.05\n",
    "beta = 1\n",
    "model = TwoLayerCorInfoMax(architecture = architecture, lambda_h = lambda_h, lambda_y = lambda_y, \n",
    "                           epsilon = epsilon, activation = activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e53ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_dynamic_iterations_free = 20\n",
    "neural_dynamic_iterations_nudged = 4\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "x, y = x.to(device), y.to(device)\n",
    "x = x.view(x.size(0),-1).T\n",
    "y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "\n",
    "y_label = y_one_hot\n",
    "\n",
    "h, y_hat = model.init_neurons(x.size(1), device = model.device)\n",
    "\n",
    "h, y_hat = model.run_neural_dynamics(x, h, y_hat, y_label, neural_lr, \n",
    "                                    neural_dynamic_iterations_free, 0, )\n",
    "neurons1 = [h, y_hat].copy()\n",
    "\n",
    "h, y_hat = model.run_neural_dynamics(x, h, y_hat, y_label, neural_lr, \n",
    "                                    neural_dynamic_iterations_nudged, beta,)\n",
    "neurons2 = [h, y_hat].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90c693a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 500])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.B[0]['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579e0bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 20])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurons2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02dd385c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1383, -0.0150,  0.0197,  0.1102, -0.0116,  0.2284,  0.2792,  0.1199,\n",
       "         0.0851,  0.2255, -0.0687,  0.0250,  0.1007,  0.0431, -0.0054,  0.1592,\n",
       "         0.0494, -0.0812,  0.0137, -0.1206,  0.2386,  0.0522,  0.0618, -0.0302,\n",
       "         0.0584,  0.4233,  0.0796, -0.1759, -0.0589,  0.2406,  0.2869,  0.2264,\n",
       "        -0.1019, -0.0474, -0.0646,  0.0281,  0.1068,  0.1232,  0.0773, -0.0943,\n",
       "         0.0597,  0.0030,  0.2092,  0.0696, -0.0195, -0.0565,  0.0768, -0.0114,\n",
       "         0.1649, -0.3336,  0.1061,  0.0980, -0.1135,  0.0232,  0.2619, -0.0636,\n",
       "        -0.0420,  0.1504,  0.2714, -0.0675,  0.1464,  0.0843,  0.0759, -0.0979,\n",
       "         0.0373,  0.0684,  0.1076, -0.0538, -0.0018,  0.1458, -0.0058, -0.0072,\n",
       "         0.0375,  0.1893,  0.2215, -0.1074, -0.0835, -0.0535, -0.0070, -0.0326,\n",
       "         0.0850,  0.2370,  0.1216, -0.1104,  0.2762,  0.0196,  0.2197,  0.0133,\n",
       "         0.3140,  0.2002,  0.2148,  0.0320,  0.6513,  0.0805,  0.1257,  0.2593,\n",
       "        -0.0165,  0.1099, -0.0959,  0.1223,  0.1720,  0.1128, -0.0205,  0.1336,\n",
       "         0.0649,  0.1523,  0.1776, -0.1023,  0.0962,  0.0677, -0.0808,  0.0221,\n",
       "         0.0786,  0.2656, -0.1360,  0.0080,  0.1838,  0.0641, -0.0378,  0.1574,\n",
       "         0.1235, -0.0685, -0.1182, -0.0791, -0.0010,  0.0257, -0.1082,  0.0284,\n",
       "         0.0491,  0.0096,  0.2362, -0.0839, -0.0153, -0.0012,  0.2303, -0.0551,\n",
       "         0.2529,  0.0253, -0.3116,  0.1486, -0.1269,  0.0684,  0.1651, -0.0607,\n",
       "        -0.1299, -0.0314,  0.0213,  0.2549,  0.2570,  0.1068,  0.2697,  0.0200,\n",
       "        -0.0062,  0.0436, -0.1440,  0.5637,  0.1435,  0.0755,  0.0259,  0.1473,\n",
       "         0.0112,  0.1798,  0.1611,  0.1006,  0.3348, -0.0984, -0.0436,  0.1938,\n",
       "         0.1108, -0.0438, -0.0787, -0.0801, -0.0116,  0.0231, -0.0542,  0.1577,\n",
       "         0.1123, -0.0412, -0.0696,  0.1769, -0.0901,  0.0385,  0.0282,  0.0287,\n",
       "         0.0227,  0.1504,  0.0065,  0.1287,  0.3069, -0.0587,  0.1133, -0.1343,\n",
       "        -0.3225,  0.0496,  0.0136, -0.1348,  0.2188, -0.1055,  0.0731,  0.1050,\n",
       "        -0.1486, -0.0512, -0.0220, -0.0042,  0.0643,  0.1183,  0.0418,  0.1536,\n",
       "         0.0560,  0.0029,  0.0547, -0.0526, -0.0501,  0.1306, -0.0668,  0.1298,\n",
       "         0.1498,  0.4363, -0.0888,  0.0538,  0.1138,  0.0200,  0.1582,  0.1670,\n",
       "         0.1306,  0.0111, -0.1577,  0.0064, -0.1216,  0.2075,  0.3614, -0.0232,\n",
       "         0.0445,  0.0769, -0.0379, -0.1846,  0.2940,  0.0377,  0.0996, -0.0856,\n",
       "         0.0064,  0.1677,  0.0933,  0.0475,  0.0644,  0.2518, -0.1281,  0.0639,\n",
       "         0.1435,  0.1437, -0.0151, -0.0706,  0.2951, -0.1721, -0.0143,  0.3447,\n",
       "         0.1401, -0.0563,  0.2456,  0.0318,  0.0888, -0.0282,  0.0657,  0.0333,\n",
       "         0.3187, -0.0376, -0.0257,  0.1041,  0.2321, -0.1865, -0.1243,  0.3015,\n",
       "         0.2493, -0.1797,  0.1387,  0.0996, -0.0374,  0.1359,  0.0282,  0.1879,\n",
       "         0.1736,  0.4706, -0.2332,  0.2079, -0.0731,  0.2183, -0.2259,  0.2186,\n",
       "        -0.2213, -0.0798,  0.0789,  0.0645,  0.1550,  0.1388,  0.1897, -0.1369,\n",
       "        -0.1513,  0.0534,  0.3672,  0.1516,  0.1946,  0.2448,  0.0880,  0.1106,\n",
       "        -0.1875,  0.1868,  0.2090, -0.1234,  0.0545, -0.1470,  0.1104,  0.1197,\n",
       "        -0.0664, -0.1217,  0.1409,  0.0332,  0.1053, -0.0526,  0.0901,  0.0064,\n",
       "         0.1255, -0.0288,  0.2012, -0.0168,  0.1244,  0.0156, -0.1186,  0.1639,\n",
       "        -0.2420, -0.0522,  0.2830, -0.1058,  0.0463, -0.1004, -0.0745,  0.3024,\n",
       "         0.1135,  0.2763, -0.1229,  0.0739,  0.1451,  0.0181,  0.1491,  0.1026,\n",
       "         0.1367, -0.0866,  0.1298,  0.1399,  0.0449,  0.1433, -0.0315,  0.0828,\n",
       "         0.1691,  0.0269,  0.0387, -0.0334, -0.0045,  0.0137,  0.1876,  0.0661,\n",
       "         0.1484,  0.1161, -0.0375,  0.0427,  0.1069, -0.0520,  0.0619,  0.0619,\n",
       "        -0.0321,  0.2755,  0.0546,  0.1017, -0.0921, -0.1137,  0.0577, -0.1241,\n",
       "         0.0499,  0.0612,  0.3511,  0.0732,  0.1775,  0.0988, -0.1265,  0.0166,\n",
       "         0.0617, -0.1370,  0.1051,  0.0342,  0.0747,  0.0414,  0.0860, -0.1198,\n",
       "         0.1754,  0.1092,  0.0582,  0.0470,  0.2308, -0.1008, -0.0740,  0.0588,\n",
       "         0.1609,  0.1930,  0.0412,  0.2015, -0.0062, -0.0278, -0.0418,  0.0162,\n",
       "         0.1788, -0.0063, -0.1354,  0.2684, -0.1001,  0.2436, -0.0133,  0.0621,\n",
       "        -0.0397,  0.2960, -0.0245, -0.0507,  0.2608,  0.4562,  0.1748, -0.0153,\n",
       "        -0.0580, -0.0895, -0.0025,  0.3315, -0.1047,  0.1235,  0.1517, -0.0682,\n",
       "         0.3865, -0.0510, -0.1766, -0.0244,  0.0232, -0.0155,  0.2380,  0.2277,\n",
       "        -0.0045, -0.0423, -0.1812, -0.0351,  0.2345, -0.0461,  0.0121, -0.1400,\n",
       "        -0.0742,  0.1288, -0.0064, -0.0571,  0.0494, -0.0379, -0.2127, -0.0799,\n",
       "         0.1192,  0.3501, -0.1373,  0.2148,  0.0158,  0.0095,  0.1911, -0.1040,\n",
       "        -0.1693,  0.3071,  0.0868,  0.0236,  0.1320,  0.2410,  0.0042,  0.2892,\n",
       "         0.0848,  0.0038, -0.1706, -0.1265, -0.0741, -0.1329,  0.0244,  0.0756,\n",
       "         0.2163,  0.0707,  0.2094,  0.0513,  0.1639, -0.0708,  0.2184, -0.0065,\n",
       "         0.4509,  0.0957, -0.1436,  0.1361,  0.1489, -0.1834, -0.1891,  0.1127,\n",
       "        -0.0032,  0.1289,  0.0991,  0.0578], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.B[0]['weight'] @ neurons2[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_acc_list = []\n",
    "tst_acc_list = []\n",
    "neural_dynamic_iterations_free = 20\n",
    "neural_dynamic_iterations_nudged = 4\n",
    "# lambda_h = 0.01\n",
    "# lambda_y = 0.01\n",
    "# epsilon = 1\n",
    "# one_over_epsilon = 1 / epsilon\n",
    "n_epochs = 50\n",
    "# lr = {'ff' : 1e-3, 'fb': 1e-3, 'lat': 1e-3}\n",
    "# neural_lr = 0.25\n",
    "\n",
    "for epoch_ in range(n_epochs):\n",
    "    for idx, (x, y) in tqdm(enumerate(train_loader)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x = x.view(x.size(0),-1).T\n",
    "        y_one_hot = F.one_hot(y, 10).to(device).T\n",
    "\n",
    "        h, y_hat = model.batch_step(  x, y_one_hot, lr, neural_lr, neural_dynamic_iterations_free, \n",
    "                                      neural_dynamic_iterations_nudged, beta, output_sparsity = True)\n",
    "\n",
    "    trn_acc = evaluateCorInfoMax(model, train_loader, neural_lr, 20, device = 'cuda', printing = False)\n",
    "    tst_acc = evaluateCorInfoMax(model, test_loader, neural_lr, 20, device = 'cuda', printing = False)\n",
    "    trn_acc_list.append(trn_acc)\n",
    "    tst_acc_list.append(tst_acc)\n",
    "    lr = {'ff' : 0.1, 'fb': 0.1, 'lat': 1e-2}\n",
    "    \n",
    "    print(\"Epoch : {}, Train Accuracy : {}, Test Accuracy : {}\".format(epoch_+1, trn_acc, tst_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e76b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([0, 9, 0, 7, 7, 3, 1, 3, 1, 5, 8, 0, 0, 1, 6, 4, 6, 5, 4, 7])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c83bd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x, y = x.to(device), y.to(device)\n",
    "x = x.view(x.size(0),-1) # flattening the input\n",
    "neurons = model.init_neurons(x.size(0), device)\n",
    "layers = [x] + neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70cc3c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 784])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W[idx].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d763bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 500])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[idx + 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f253000a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b255a5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 500])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W[idx](layers[idx]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "955d6e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 500])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.W[idx](layers[idx]) - layers[idx + 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "092af5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.9167,  9.3459,  6.2711,  7.1550,  8.3149,  6.3540,  7.4546,  6.7009,\n",
       "         7.8377,  6.9372,  7.8099,  5.9722,  7.2506,  7.0621, 10.4032,  8.6917,\n",
       "         6.2146,  6.4138,  7.8474, 11.1637], device='cuda:0',\n",
       "       grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "torch.norm(model.W[idx](layers[idx]) - layers[idx + 1], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7a19919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_204844/3112052343.py:1: UserWarning: torch.matrix_rank is deprecated in favor of torch.linalg.matrix_rankand will be removed in a future PyTorch release. The parameter 'symmetric' was renamed in torch.linalg.matrix_rank to 'hermitian'. (Triggered internally at  ../aten/src/ATen/native/LinearAlgebra.cpp:618.)\n",
      "  torch.matrix_rank(model.M[0].weight.data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(495, device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(model.M[0].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37911978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 500])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.randn(20, 500, requires_grad = True)\n",
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12ffb0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1250, -0.1152,  0.0715,  ..., -0.1319, -0.3065,  0.1999],\n",
       "        [-0.1152,  1.0950, -0.1908,  ..., -0.3904, -0.2277, -0.1993],\n",
       "        [ 0.0715, -0.1908,  0.6777,  ...,  0.0735,  0.3597,  0.0956],\n",
       "        ...,\n",
       "        [-0.1319, -0.3904,  0.0735,  ...,  1.3210,  0.1744, -0.1298],\n",
       "        [-0.3065, -0.2277,  0.3597,  ...,  0.1744,  1.4838, -0.0350],\n",
       "        [ 0.1999, -0.1993,  0.0956,  ..., -0.1298, -0.0350,  1.1093]],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(aa[..., None] * aa[:, None]).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76ab30d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "outer: Expected 1-D argument self, but got 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_204844/2542316666.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: outer: Expected 1-D argument self, but got 2-D"
     ]
    }
   ],
   "source": [
    "torch.outer(layers[1], layers[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
